{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "42561c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from ndlinear import NdLinear\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "nt_to_idx = {'A': 0, 'U': 1, 'G': 2, 'C': 3, 'N': 4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5a6ed86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNAStruct(Dataset):\n",
    "    def __init__(self, seq_csv, labels_csv, mean=None, std=None, normalize=True):\n",
    "        seq_df = pd.read_csv(seq_csv)\n",
    "        seq_map = {row['target_id']: row['sequence'].strip() for _, row in seq_df.iterrows()}\n",
    "\n",
    "        labels_df = pd.read_csv(labels_csv)\n",
    "        self.data = {}\n",
    "        for _, row in labels_df.iterrows():\n",
    "            target_id = row['ID'].rsplit('_', 1)[0]\n",
    "            idx = int(row['resid']) - 1\n",
    "            if target_id not in self.data:\n",
    "                self.data[target_id] = []\n",
    "            self.data[target_id].append((idx, row['resname'], row['x_1'], row['y_1'], row['z_1']))\n",
    "\n",
    "        self.samples = []\n",
    "        for target_id in self.data:\n",
    "            dat = sorted(self.data[target_id], key=lambda x: x[0])\n",
    "            seq = seq_map[target_id]\n",
    "            coords = np.array([[x[2], x[3], x[4]] for x in dat], dtype=np.float32)\n",
    "            if len(seq) == len(coords) and np.isfinite(coords).all():\n",
    "                seq_idx = np.array([nt_to_idx.get(nt, 4) for nt in seq], dtype=np.int64)\n",
    "                self.samples.append((target_id, seq_idx, coords))\n",
    "            else:\n",
    "                print(f\"Bad entry removed: {target_id} (len/coords/finite)\")\n",
    "\n",
    "        all_coords = np.concatenate([coords for _, _, coords in self.samples], axis=0)\n",
    "        self.mean = mean if mean is not None else all_coords.mean(axis=0)\n",
    "        self.std = std if std is not None else all_coords.std(axis=0)\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        target_id, seq_idx, coords = self.samples[idx]\n",
    "        if self.normalize:\n",
    "            norm_coords = (coords - self.mean) / self.std\n",
    "            return (\n",
    "                torch.LongTensor(seq_idx),\n",
    "                torch.tensor(norm_coords, dtype=torch.float32),\n",
    "                target_id\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                torch.LongTensor(seq_idx),\n",
    "                torch.tensor(coords, dtype=torch.float32),\n",
    "                target_id\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9b051e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train_model(model, dataloader, device, epochs=100, lr=1e-3, val_loader=None, scheduler=None):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss(reduction='none')\n",
    "    best_val = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_points = 0\n",
    "        for seqs, coords, lengths, _ in dataloader:\n",
    "            seqs, coords, lengths = seqs.to(device), coords.to(device), lengths.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred_coords = model(seqs, lengths)\n",
    "            mask = torch.arange(seqs.size(1), device=device)[None, :] < lengths[:, None]\n",
    "            mse = loss_fn(pred_coords, coords).sum(2)\n",
    "            loss = (mse * mask).sum() / mask.sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * mask.sum().item()\n",
    "            total_points += mask.sum().item()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        print(f\"Epoch {epoch+1} Loss: {running_loss/total_points:.5f}\")\n",
    "        if val_loader is not None:\n",
    "            val_rmsd = validate(model, val_loader, device)\n",
    "            if val_rmsd < best_val:\n",
    "                best_val = val_rmsd\n",
    "                torch.save(model.state_dict(), 'best_model.pt')\n",
    "                print(f\"New BEST model saved at Epoch {epoch+1} (RMSD: {val_rmsd:.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6fdff36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collate function\n",
    "def rna_collate(batch):\n",
    "    seqs, coords, lengths, ids = zip(*batch)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    seqs_padded = pad_sequence(seqs, batch_first=True, padding_value=4)\n",
    "    coords_padded = pad_sequence(coords, batch_first=True, padding_value=0)\n",
    "    return seqs_padded, coords_padded, lengths, ids\n",
    "\n",
    "\n",
    "# Model\n",
    "class RNA3DNet(nn.Module):\n",
    "    def __init__(self, vocab_size=5, emb_dim=128, num_layers=6, nhead=8, ff_dim=256, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=4)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_dim, nhead=nhead, dim_feedforward=ff_dim,\n",
    "            dropout=dropout, batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.ln = nn.LayerNorm(emb_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(emb_dim, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, seq, lengths):\n",
    "        x = self.embed(seq)\n",
    "        mask = (seq == 4) \n",
    "        x = self.encoder(x, src_key_padding_mask=mask)\n",
    "        x = self.ln(x)\n",
    "        coords = self.fc(x)\n",
    "        return coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "203ea1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_pts, total_rmsd = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for seqs, coords, lengths, _ in dataloader:\n",
    "            seqs, coords, lengths = seqs.to(device), coords.to(device), lengths.to(device)\n",
    "            pred_coords = model(seqs, lengths)\n",
    "            mask = torch.arange(seqs.size(1), device=device)[None, :] < lengths[:, None]\n",
    "            diff = ((pred_coords - coords) ** 2).sum(2).sqrt()\n",
    "            rmsd = (diff * mask).sum() / mask.sum()\n",
    "            total_rmsd += rmsd.item() * mask.sum().item()\n",
    "            total_pts += mask.sum().item()\n",
    "    mean_rmsd = total_rmsd / total_pts\n",
    "    print(f\"Validation RMSD: {mean_rmsd:.4f}\")\n",
    "    model.train()\n",
    "    return mean_rmsd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "671a9124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train_model(model, dataloader, device, epochs=50, lr=3e-4, val_loader=None):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    \n",
    "\n",
    "    total_steps = len(dataloader) * epochs\n",
    "    warmup_steps = min(1000, total_steps // 10) \n",
    "    \n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        return 1.0\n",
    "    \n",
    "    device_type = 'cuda' if device.type == 'cuda' else 'cpu'\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    mse_loss = nn.MSELoss(reduction='none')\n",
    "    best_val_rmsd = float('inf')\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_points = 0\n",
    "        \n",
    "        for seqs, coords, lengths, _ in dataloader:\n",
    "            seqs, coords, lengths = seqs.to(device), coords.to(device), lengths.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.amp.autocast(device_type=device_type):\n",
    "                pred_coords = model(seqs, lengths)\n",
    "                mask = torch.arange(seqs.size(1), device=device)[None, :] < lengths[:, None]\n",
    "                mse = mse_loss(pred_coords, coords).sum(2)\n",
    "                loss = (mse * mask).sum() / mask.sum()\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            running_loss += loss.item() * mask.sum().item()\n",
    "            total_points += mask.sum().item()\n",
    "        \n",
    "            \n",
    "        epoch_loss = running_loss / total_points\n",
    "        print(f\"Epoch {epoch+1} Loss: {epoch_loss:.5f}\")\n",
    "        \n",
    "        if val_loader is not None:\n",
    "            val_rmsd = validate(model, val_loader, device)\n",
    "            print(f\"Validation RMSD: {val_rmsd:.5f}\")\n",
    "\n",
    "            scheduler.step(val_rmsd)\n",
    "\n",
    "            if val_rmsd < best_val_rmsd:\n",
    "                best_val_rmsd = val_rmsd\n",
    "                torch.save(model.state_dict(), 'best_model.pt')\n",
    "                print(f\"New best checkpoint (epoch {epoch+1}, RMSD {val_rmsd:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f4c62a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submission Function\n",
    "def submission(model, seq_csv, submission_csv, device, mean, std):\n",
    "    seq_df = pd.read_csv(seq_csv)\n",
    "    model.eval()\n",
    "    submissions = []\n",
    "    for idx, row in seq_df.iterrows():\n",
    "        target_id, sequence = row['target_id'], row['sequence'].strip()\n",
    "        seq_idx = torch.LongTensor([nt_to_idx.get(nt, 4) for nt in sequence]).unsqueeze(0).to(device)\n",
    "        lengths = torch.tensor([len(sequence)]).to(device)\n",
    "        with torch.no_grad():\n",
    "            coords = model(seq_idx, lengths)[0][:len(sequence)].cpu().numpy()\n",
    "            coords = coords * std + mean\n",
    "        for i, (nt, (x, y, z)) in enumerate(zip(sequence, coords)):\n",
    "            id_out = f\"{target_id}_{i+1}\"\n",
    "            submissions.append({\n",
    "                \"ID\": id_out,\n",
    "                \"resname\": nt,\n",
    "                \"resid\": i+1,\n",
    "                \"x_1\": x, \"y_1\": y, \"z_1\": z,\n",
    "                \"x_2\": 0.0, \"y_2\": 0.0, \"z_2\": 0.0,\n",
    "                \"x_3\": 0.0\n",
    "            })\n",
    "    sub_df = pd.DataFrame(submissions)\n",
    "    sub_df.to_csv(submission_csv, index=False)\n",
    "    print(f\"Submission CSV written: {submission_csv}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6f19a566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pradip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m     17\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(optimizer, T_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     23\u001b[0m submission(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrna-folding/test_sequences.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrna-folding/submission.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, device, mean, std)\n",
      "Cell \u001b[1;32mIn[73], line 27\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, device, epochs, lr, val_loader)\u001b[0m\n\u001b[0;32m     24\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     25\u001b[0m total_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 27\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseqs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pradip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Pradip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Pradip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[71], line 3\u001b[0m, in \u001b[0;36mrna_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrna_collate\u001b[39m(batch):\n\u001b[1;32m----> 3\u001b[0m     seqs, coords, lengths, ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m      4\u001b[0m     lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(lengths)\n\u001b[0;32m      5\u001b[0m     seqs_padded \u001b[38;5;241m=\u001b[39m pad_sequence(seqs, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "#Training \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = RNAStruct(\"rna-folding/train_sequences_clean.csv\", \"rna-folding/train_labels_clean.csv\")\n",
    "val_dataset = RNAStruct(\"rna-folding/validation_sequences.csv\", \"rna-folding/validation_labels.csv\")\n",
    "\n",
    "mean = train_dataset.mean\n",
    "std = train_dataset.std\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=rna_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=rna_collate)\n",
    "\n",
    "model = RNA3DNet().to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "train_model(model, train_loader, device, epochs=30, lr=1e-5, val_loader=val_loader)\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "submission(model, \"rna-folding/test_sequences.csv\", \"rna-folding/submission.csv\", device, mean, std)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
